{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fetching Twitter data - Network analysis project","provenance":[{"file_id":"1eAhYHOk2k0irUQjISDDqVby2vcaVbEHW","timestamp":1581621267504}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"IVRqpg_ah2GE","colab_type":"code","outputId":"79846139-0b20-46cd-9c2e-bbd2d0a28ad8","executionInfo":{"status":"ok","timestamp":1582310148558,"user_tz":420,"elapsed":506,"user":{"displayName":"Nick Cottrell","photoUrl":"","userId":"11106463128379382840"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\"\n","To run as a standalone script, set your CONSUMER_KEY and CONSUMER_SECRET. To\n","call search from code, pass in your credentials to the search_twitter function.\n","\n","Script to fetch a twitter search of tweets into a directory. Fetches all available\n","tweet history accessible by the application (7 days historical).\n","\n","## Operation\n","\n","Search fetches tweets in pages of 100 from the most recent tweet backwards.\n","Thus, you could fetch just the most recent few by interrupting the script at\n","any time.\n","\n","By default tweets will be fetched into a zip file containing one .json file per\n","tweets. The --nozip flag will result in .json files being writting directly to\n","the output directory.\n","\n","## Subsequent search execution\n","\n","In case of interrupted searches, you may continue where you left off:\n","\n","On subsequent runs of the same query, search will check for existing tweets in\n","the output directory and will pick up where it left off at the lowest tweet ID,\n","and again work backwards in pages through the remaining history.\n","\n","Thus, in order to execute a full query from scratch, be sure to remove any\n","existing tweets from the relevant output directory -- but note that some of the\n","oldest tweets may no longer be available for a fresh search.\n","\n","During subsequent runs of a query you may also use the --new flag wich will\n","cause the search to only fetch tweets newer than those currently in the\n","output directory.\n","\n","Search will throttle at 440 requests per 15 minutes to keep it safely under the\n","designated 450 allowed as per the Twitter docs here:\n","https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html\n","\"\"\""],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nTo run as a standalone script, set your CONSUMER_KEY and CONSUMER_SECRET. To\\ncall search from code, pass in your credentials to the search_twitter function.\\n\\nScript to fetch a twitter search of tweets into a directory. Fetches all available\\ntweet history accessible by the application (7 days historical).\\n\\n## Operation\\n\\nSearch fetches tweets in pages of 100 from the most recent tweet backwards.\\nThus, you could fetch just the most recent few by interrupting the script at\\nany time.\\n\\nBy default tweets will be fetched into a zip file containing one .json file per\\ntweets. The --nozip flag will result in .json files being writting directly to\\nthe output directory.\\n\\n## Subsequent search execution\\n\\nIn case of interrupted searches, you may continue where you left off:\\n\\nOn subsequent runs of the same query, search will check for existing tweets in\\nthe output directory and will pick up where it left off at the lowest tweet ID,\\nand again work backwards in pages through the remaining history.\\n\\nThus, in order to execute a full query from scratch, be sure to remove any\\nexisting tweets from the relevant output directory -- but note that some of the\\noldest tweets may no longer be available for a fresh search.\\n\\nDuring subsequent runs of a query you may also use the --new flag wich will\\ncause the search to only fetch tweets newer than those currently in the\\noutput directory.\\n\\nSearch will throttle at 440 requests per 15 minutes to keep it safely under the\\ndesignated 450 allowed as per the Twitter docs here:\\nhttps://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html\\n'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"smm1Jzzc-bV6","colab_type":"code","outputId":"9bd6b5a4-8ba2-4608-87c8-abdcfc64b3c8","executionInfo":{"status":"ok","timestamp":1582310176496,"user_tz":420,"elapsed":28426,"user":{"displayName":"Nick Cottrell","photoUrl":"","userId":"11106463128379382840"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VMagLu4Kkrad","colab_type":"code","outputId":"a236d4d7-3539-4e7b-ef49-9a48e3e4413f","executionInfo":{"status":"ok","timestamp":1582310184483,"user_tz":420,"elapsed":36394,"user":{"displayName":"Nick Cottrell","photoUrl":"","userId":"11106463128379382840"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["!pip install birdy\n","!pip install ratelimiter\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting birdy\n","  Downloading https://files.pythonhosted.org/packages/cc/30/3f825b8d4248ebd9de9d218ba4b931c93be664e077c328c4b6dd19eb9d8a/birdy-0.3.2.tar.gz\n","Requirement already satisfied: requests>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from birdy) (2.21.0)\n","Requirement already satisfied: requests_oauthlib>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from birdy) (1.3.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.3->birdy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.3->birdy) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.3->birdy) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.3->birdy) (2.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests_oauthlib>=0.3.2->birdy) (3.1.0)\n","Building wheels for collected packages: birdy\n","  Building wheel for birdy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for birdy: filename=birdy-0.3.2-cp36-none-any.whl size=10853 sha256=6ce83512f97bbe4690e27e0f39179fbea926b3afe99500dd4c198d646f059c50\n","  Stored in directory: /root/.cache/pip/wheels/ad/f9/a7/928ef99a65cfa8182e42fb0a052b0a61faa69b7d085fae2723\n","Successfully built birdy\n","Installing collected packages: birdy\n","Successfully installed birdy-0.3.2\n","Collecting ratelimiter\n","  Downloading https://files.pythonhosted.org/packages/51/80/2164fa1e863ad52cc8d870855fba0fbb51edd943edffd516d54b5f6f8ff8/ratelimiter-1.2.0.post0-py3-none-any.whl\n","Installing collected packages: ratelimiter\n","Successfully installed ratelimiter-1.2.0.post0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XkXtbAk2hLdl","colab_type":"code","colab":{}},"source":["import json, os, sys, time\n","from zipfile import ZipFile\n","from birdy.twitter import AppClient, UserClient, TwitterRateLimitError\n","from ratelimiter import RateLimiter\n","\n","\n","\"\"\"\n","Credentials can be found by selecting the \"Keys and tokens\" tab for your\n","application selected from:\n","\n","https://developer.twitter.com/en/apps/\n","\"\"\"\n","CONSUMER_KEY = 'fyzZfPI1BYmz788aesPLmsrY3'\n","CONSUMER_SECRET = 'lQ3WnI3Hjm70jByOK5MpgirJpfM3EEVMIpf4jbEQcrWWZTBB6N'\n","\n","# need to spcify output \n","OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/Network Analysis Project'\n","MAX_TWEETS = 10000 # max results for a search\n","max_id = None\n","_client = None\n","\n","\n","def client(consumer_key=None, consumer_secret=None):\n","    global _client\n","    if consumer_key is None:\n","        consumer_key = CONSUMER_KEY\n","    if consumer_secret is None:\n","        consumer_secret = CONSUMER_SECRET\n","    if _client is None:\n","        _client = AppClient(consumer_key, consumer_secret)\n","        access_token = _client.get_access_token()\n","        _client = AppClient(consumer_key, consumer_secret, access_token)\n","    return _client\n","\n","\n","def limited(until):\n","    duration = int(round(until - time.time()))\n","    print('Rate limited, sleeping for {:d} seconds'.format(duration))\n","\n","\n","@RateLimiter(max_calls=440, period=60*15, callback=limited)\n","def fetch_tweets(query, consumer_key=None, consumer_secret=None):\n","    global max_id\n","    print(f'Fetching: \"{query}\" TO MAX ID: {max_id}')\n","    try:\n","        tweets = client(consumer_key, consumer_secret).api.search.tweets.get(\n","            q=query,\n","            count=100,\n","            max_id=max_id).data['statuses']\n","    except TwitterRateLimitError:\n","        sys.exit(\"You've reached your Twitter API rate limit. \"\\\n","            \"Wait 15 minutes before trying again\")\n","    try:\n","        id_ = min([tweet['id'] for tweet in tweets])\n","    except ValueError:\n","        return None\n","    if max_id is None or id_ <= max_id:\n","        max_id = id_ - 1\n","    return tweets\n","\n","\n","def initialize_max_id(file_list):\n","    global max_id\n","    for fn in file_list:\n","        n = int(fn.split('.')[0])\n","        if max_id is None or n < max_id:\n","            max_id = n - 1\n","    if max_id is not None:\n","        print('Found previously fetched tweets. Setting max_id to %d' % max_id)\n","\n","\n","def halt(_id):\n","    print('Reached historically fetched ID: %d' % _id)\n","    print('In order to re-fetch older tweets, ' \\\n","        'remove tweets from the output directory or output zip file.')\n","    sys.exit('\\n!!IMPORTANT: Tweets older than 7 days will not be re-fetched')\n","\n","\n","def search_twitter(query, consumer_key=None, consumer_secret=None,\n","            newtweets=False, dozip=True, verbose=False):\n","    output_dir = os.path.join(OUTPUT_DIR, '_'.join(query.split()))\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    if dozip:\n","        fn = os.path.join(output_dir, '%s.zip' % '_'.join(query.split()))\n","        outzip = ZipFile(fn, 'a')\n","    if not newtweets:\n","        if dozip:\n","            file_list = [f for f in outzip.namelist() if f.endswith('.json')]\n","        else:\n","            file_list = [f for f in os.listdir(output_dir) if f.endswith('.json')]\n","        initialize_max_id(file_list)\n","    while True:\n","        try:\n","            tweets = fetch_tweets(\n","                query,\n","                consumer_key=consumer_key,\n","                consumer_secret=consumer_secret)\n","            if tweets is None:\n","                print('Search Completed')\n","                if dozip:\n","                    outzip.close()\n","                break\n","            for tweet in tweets:\n","                if verbose:\n","                    print(tweet['id'])\n","                fn = '%d.json' % tweet['id']\n","                if dozip:\n","                    if fn in (file_list):\n","                        outzip.close()\n","                        halt(tweet['id'])\n","                    else:\n","                        outzip.writestr(fn, json.dumps(tweet, indent=4))\n","                        file_list.append(fn)\n","                else:\n","                    path = os.path.join(output_dir, fn)\n","                    if fn in (file_list):\n","                        halt(tweet['id'])\n","                    else:\n","                        with open(path, 'w') as outfile:\n","                            json.dump(tweet, outfile, indent=4)\n","                        file_list.append(fn)\n","                if len(file_list) >= MAX_TWEETS:\n","                    if fn in (file_list):\n","                        outzip.close()\n","                    # sys.exit('Reached maximum tweet limit of: %d' % MAX_TWEETS)\n","        except:\n","            if dozip:\n","                outzip.close()\n","            raise"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KyEOcZr6lPXd","colab_type":"code","outputId":"3743a077-f27d-4730-8d1c-b49b10f96f76","executionInfo":{"status":"ok","timestamp":1582319063392,"user_tz":420,"elapsed":19806,"user":{"displayName":"Nick Cottrell","photoUrl":"","userId":"11106463128379382840"}},"colab":{"base_uri":"https://localhost:8080/","height":952}},"source":["search_twitter('Colorado Avalanche')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Fetching: \"Colorado Avalanche\" TO MAX ID: None\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1230725424369434629\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1230628482490798079\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1230474439780794368\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1230328751130632198\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1230256787338731522\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1230198045909880831\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1230130791687938047\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229923671021621248\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229886504035594242\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229858456267567106\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229794678481932287\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229733563437809664\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229632397232267263\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229595164001423360\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229581673634947073\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229552975405834240\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229495902986997759\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229441903227097087\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229409886007517184\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229368123436027903\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229320043948888064\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229264290567401471\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229239048495149057\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229216705509044223\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229196144615608319\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229178673263185921\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229165060670607361\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229157510851878911\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229156803105247233\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229149796436529154\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229118855789805567\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1229052954998538240\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228918565979377663\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228889462337179647\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228862966830731268\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228851528691277823\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228833376397856767\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228790401600909311\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228757393288331263\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228725747671539713\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228551756532944897\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228410791461699585\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228341892129095680\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228189530127093759\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228147422875901952\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228095462252539905\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228051911992987647\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1228005128688500735\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1227882015778889727\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1227736249235951615\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1227581169245589503\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1227457018665033729\n","Fetching: \"Colorado Avalanche\" TO MAX ID: 1227345773773410303\n","Search Completed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pVspK3QyB1Lz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"dcb0bd2a-d1fb-4c6c-9e48-bea57060137c","executionInfo":{"status":"error","timestamp":1582311895577,"user_tz":420,"elapsed":37740,"user":{"displayName":"Nick Cottrell","photoUrl":"","userId":"11106463128379382840"}}},"source":["search_twitter('St. Louis Blues')\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Fetching: \"St. Louis Blues\" TO MAX ID: None\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1230820770198294527\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1230692890415968256\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1230635655794954239\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1230514707854852095\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1230263872952684543\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1230194972453937152\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1230105910959378434\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229980592923652095\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229950009317216258\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229927638392852479\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229905478886608899\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229889429508173823\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229881749280370687\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229875215745744896\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229873235124260863\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229862299596312575\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229844237459623935\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229833713988624385\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229825072547926018\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229817472804474879\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229812562583179263\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229809419241885696\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229807322261684223\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229804772431187973\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229803884178268159\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229653542257848319\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229440116432736256\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229263286102786047\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229198722455810049\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229175171841347583\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1229096738712981503\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228937848524541953\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228829221381967872\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228787106132242432\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228757405623865344\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228702976602185731\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228563314847948799\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228496606355480580\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228453021232685060\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228416636899807231\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228398302028337152\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228376394079653887\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228357459204616192\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228306584582967297\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228204005836214273\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228176451087695871\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228154287852806143\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228102438844059648\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228068530928705571\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1228013387885547522\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227927827909091328\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227807671828480019\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227779381017432063\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227750714010722305\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227732906405220352\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227722153702952960\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227702570208124927\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227678447167606783\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227658047318704127\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227646854826991615\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227637538623512579\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227626963977822212\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227620091610763263\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227612273683755010\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227606995454304255\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227602117185167359\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227599259089817601\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227595089200939007\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227589333076578303\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227582951564730367\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227575484134895615\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227567025276571647\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227559979617484799\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227551361685999615\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227538967119060991\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227524611467857919\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227511620521414655\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227499033108738048\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227489269251313663\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227482149269524480\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227477433416323071\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227473259215626240\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227470405486686208\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227468048682098687\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227466353323519999\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227464177175973887\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227461985945686016\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227460264993443841\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227458004259397637\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227455562998276099\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227453564085248005\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227452291978006528\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227450971254640640\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227450319518433279\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227449138037563392\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227447418935029765\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227446306139975681\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227445277767892998\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227444617177632769\n","Fetching: \"St. Louis Blues\" TO MAX ID: 1227444140595712006\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-6bf240358b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch_twitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'St. Louis Blues'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-72c8e6ab71b1>\u001b[0m in \u001b[0;36msearch_twitter\u001b[0;34m(query, consumer_key, consumer_secret, newtweets, dozip, verbose)\u001b[0m\n\u001b[1;32m    109\u001b[0m                         \u001b[0mhalt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                         \u001b[0moutzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                         \u001b[0mfile_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mwritestr\u001b[0;34m(self, zinfo_or_arcname, data, compress_type)\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1670\u001b[0;31m                 \"Attempt to write to ZIP archive that was already closed\")\n\u001b[0m\u001b[1;32m   1671\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m             raise ValueError(\n","\u001b[0;31mValueError\u001b[0m: Attempt to write to ZIP archive that was already closed"]}]},{"cell_type":"code","metadata":{"id":"7bOXluzsB4h9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":816},"outputId":"b8350fd0-d502-4da6-a111-a611279beae6","executionInfo":{"status":"ok","timestamp":1582320104826,"user_tz":420,"elapsed":16465,"user":{"displayName":"Nick Cottrell","photoUrl":"","userId":"11106463128379382840"}}},"source":["search_twitter('Dallas Stars')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Fetching: \"Dallas Stars\" TO MAX ID: None\n","Fetching: \"Dallas Stars\" TO MAX ID: 1230856054965231615\n","Fetching: \"Dallas Stars\" TO MAX ID: 1230658726006317055\n","Fetching: \"Dallas Stars\" TO MAX ID: 1230573198699352063\n","Fetching: \"Dallas Stars\" TO MAX ID: 1230507131369029631\n","Fetching: \"Dallas Stars\" TO MAX ID: 1230355349837942784\n","Fetching: \"Dallas Stars\" TO MAX ID: 1230326845482115072\n","Fetching: \"Dallas Stars\" TO MAX ID: 1230307709444067328\n","Fetching: \"Dallas Stars\" TO MAX ID: 1230265667053596671\n","Fetching: \"Dallas Stars\" TO MAX ID: 1230212238180593663\n","Fetching: \"Dallas Stars\" TO MAX ID: 1230162464161062912\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229959343329574911\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229846163257483264\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229672427220656128\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229531018606018562\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229436355949649919\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229256371855556607\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229220776953421830\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229186240861417472\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229152219980816383\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229099326598717439\n","Fetching: \"Dallas Stars\" TO MAX ID: 1229052755333001215\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228939825094459396\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228892287259881472\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228876219556429824\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228856377122197505\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228832652666589184\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228757169580974079\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228568915602747391\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228465593738723327\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228388690822672383\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228357379982536704\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228312696329789441\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228185326721519632\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228145205553061887\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228123335147606015\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228108527618678783\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228078703244140546\n","Fetching: \"Dallas Stars\" TO MAX ID: 1228024813245095938\n","Fetching: \"Dallas Stars\" TO MAX ID: 1227986931738300415\n","Fetching: \"Dallas Stars\" TO MAX ID: 1227860946560933888\n","Fetching: \"Dallas Stars\" TO MAX ID: 1227719387563593730\n","Fetching: \"Dallas Stars\" TO MAX ID: 1227626196545859584\n","Fetching: \"Dallas Stars\" TO MAX ID: 1227511051350138879\n","Fetching: \"Dallas Stars\" TO MAX ID: 1227420634260443135\n","Fetching: \"Dallas Stars\" TO MAX ID: 1227351722193977349\n","Search Completed\n"],"name":"stdout"}]}]}